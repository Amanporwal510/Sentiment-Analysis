{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c9a8576",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def tokenize(sentence, remove_vowels=False, remove_repeats=False, min_len=2):\n",
    "    tokens = []\n",
    "    for token in re.findall(\"[a-zA-Z]+\",sentence.lower()):\n",
    "\n",
    "        if len(token) >= min_len:\n",
    "            if remove_vowels:\n",
    "                token = delete_vowels(token)\n",
    "            if remove_repeats:\n",
    "                token = delete_repeats(token)\n",
    "            tokens.append(token)\n",
    "    return tokens\n",
    "\n",
    "VOWELS = ['a', 'e', 'i', 'o', 'u']\n",
    "\n",
    "def delete_repeats(string):\n",
    "    return re.sub(r'(.)\\1+', r'\\1\\1', string)     \n",
    "\n",
    "def delete_vowels(string):\n",
    "    return ''.join([l for l in string.lower() if l not in VOWELS])\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    pass\n",
    "\n",
    "def normalize_matrix(matrix):\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4fc15a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import pickle\n",
    "from copy import deepcopy\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing import sequence\n",
    "from keras import backend as K\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import LSTM, GRU, Bidirectional\n",
    "from keras.layers.convolutional import Convolution1D, MaxPooling1D\n",
    "\n",
    "from keras.utils import np_utils\n",
    "from keras.utils import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98633fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filenames\n",
    "Masterdir = 'C:/Users/aman porwal/Desktop/Sentiment-Analysis-BidirectionalLSTM/'\n",
    "Datadir = 'Data/'\n",
    "inputdatasetfilename = 'tweetsData.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fad3e0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data I/O formatting\n",
    "SEPERATOR = '\\t'\n",
    "DATA_COLUMN = 1\n",
    "LABEL_COLUMN = 3\n",
    "LABELS = ['0','1','2'] # 0 -> Negative, 1-> Neutral, 2-> Positive\n",
    "mapping_char2num = {}\n",
    "mapping_num2char = {}\n",
    "MAXLEN = 200\n",
    "\n",
    "#LSTM Model Parameters\n",
    "#Embedding\n",
    "MAX_FEATURES = 0\n",
    "embedding_size = 128\n",
    "\n",
    "# Convolution\n",
    "filter_length = 3\n",
    "nb_filter = 128\n",
    "pool_length = 3\n",
    "\n",
    "# LSTM\n",
    "lstm_output_size = 128\n",
    "\n",
    "# Training\n",
    "batch_size = 128\n",
    "number_of_epochs = 1\n",
    "numclasses = 3\n",
    "test_size = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d69bc48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseData(Masterdir,filename,seperator,datacol,labelcol,labels):\n",
    "    f=open(Masterdir+Datadir+filename,'r', encoding=\"utf-8\")\n",
    "    lines = f.read().lower()\n",
    "    lines = lines.lower().split('\\n')[:-1]\n",
    "\n",
    "    X_train = []\n",
    "    Y_train = []\n",
    "    \n",
    "    for line in lines:\n",
    "        line = line.split(seperator)\n",
    "        tokenized_lines = tokenize(line[datacol])\n",
    "        \n",
    "        char_list = []\n",
    "        for words in tokenized_lines:\n",
    "            for char in words:\n",
    "                char_list.append(char)\n",
    "            char_list.append(' ')\n",
    "        X_train.append(char_list)\n",
    "        \n",
    "        if line[labelcol] == labels[0]:\n",
    "            Y_train.append(0)\n",
    "        if line[labelcol] == labels[1]:\n",
    "            Y_train.append(1)\n",
    "        if line[labelcol] == labels[2]:\n",
    "            Y_train.append(2)\n",
    "\n",
    "    Y_train = np.asarray(Y_train)\n",
    "    assert(len(X_train) == Y_train.shape[0])\n",
    "\n",
    "    return [X_train,Y_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4a6450a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_chartonum(mapping_n2c,mapping_c2n,trainwords,maxlen):\n",
    "    allchars = []\n",
    "    errors = 0\n",
    "\n",
    "    for line in trainwords:\n",
    "        try:\n",
    "            allchars = set(allchars+line)\n",
    "            allchars = list(allchars)\n",
    "        except:\n",
    "            errors += 1\n",
    "\n",
    "    charno = 0\n",
    "    for char in allchars:\n",
    "        mapping_char2num[char] = charno\n",
    "        mapping_num2char[charno] = char\n",
    "        charno += 1\n",
    "\n",
    "    assert(len(allchars)==charno) #Checks\n",
    "\n",
    "    X_train = []\n",
    "    for line in trainwords:\n",
    "        char_list=[]\n",
    "        for letter in line:\n",
    "            char_list.append(mapping_char2num[letter])\n",
    "        X_train.append(char_list)\n",
    "    print(mapping_char2num)\n",
    "    print(mapping_num2char)\n",
    "\n",
    "    X_train = pad_sequences(X_train[:], maxlen=maxlen)\n",
    "    return [X_train,mapping_num2char,mapping_char2num,charno]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85d8403c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def BidirectionalLSTM(X_train,y_train,args):\n",
    "    max_features = args[0]\n",
    "    maxlen = args[1]\n",
    "    embedding_size = args[2]\n",
    "    \n",
    "    # Convolution hyperparameters\n",
    "    filter_length = args[3]\n",
    "    nb_filter = args[4]\n",
    "    pool_length = args[5]\n",
    "    \n",
    "    # LSTM hyperparameters\n",
    "    lstm_output_size = args[6]\n",
    "    \n",
    "    # Training hyperparameters\n",
    "    batch_size = args[7]\n",
    "    nb_epoch = args[8]\n",
    "    numclasses = args[9]\n",
    "    test_size = args[10] \n",
    "\n",
    "    y_train = np_utils.to_categorical(y_train, numclasses) \n",
    "    \n",
    "    #Train & Validation data splitting\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=test_size, random_state=42)\n",
    "    \n",
    "    #Build the sequential model\n",
    "    # Model Architecture is:\n",
    "    # Input -> Embedding -> Conv1D+Maxpool1D -> BidirectionalLSTM -> FC-1 -> Softmaxloss\n",
    "    print('Build model...')\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(max_features, embedding_size, input_length=maxlen))\n",
    "    model.add(Convolution1D(filters=nb_filter, kernel_size=filter_length, padding='valid', activation='relu', strides=1))\n",
    "    model.add(MaxPooling1D(pool_size=pool_length))\n",
    "    model.add(Bidirectional(LSTM(lstm_output_size, dropout=0.2, recurrent_dropout=0.2, return_sequences=True), merge_mode='concat'))\n",
    "    model.add(Bidirectional(LSTM(lstm_output_size, dropout=0.2, recurrent_dropout=0.2)))\n",
    "    model.add(Dense(numclasses))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    # Optimizer is Adamax along with categorical crossentropy loss\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adamax', metrics=['accuracy'])\n",
    "\n",
    "    print('Train...')\n",
    "    model.fit(X_train, y_train, batch_size=batch_size, shuffle=True, epochs=nb_epoch, validation_data=(X_valid, y_valid))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0394eb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(X_test,y_test,model,batch_size,numclasses):\n",
    "    #Convert y_test to one-hot encoding\n",
    "    y_test = np_utils.to_categorical(y_test, numclasses)\n",
    "    #Evaluate the accuracies\n",
    "    score, acc = model.evaluate(X_test, y_test, batch_size=batch_size)\n",
    "    print('Test score:', score)\n",
    "    print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998ce785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'x': 0, 'o': 1, 'f': 2, 'e': 3, 'j': 4, 'a': 5, ' ': 6, 'q': 7, 'k': 8, 'i': 9, 'u': 10, 'g': 11, 'd': 12, 'c': 13, 'w': 14, 'h': 15, 's': 16, 'n': 17, 'm': 18, 'r': 19, 'p': 20, 't': 21, 'v': 22, 'l': 23, 'y': 24, 'z': 25, 'b': 26}\n",
      "{0: 'x', 1: 'o', 2: 'f', 3: 'e', 4: 'j', 5: 'a', 6: ' ', 7: 'q', 8: 'k', 9: 'i', 10: 'u', 11: 'g', 12: 'd', 13: 'c', 14: 'w', 15: 'h', 16: 's', 17: 'n', 18: 'm', 19: 'r', 20: 'p', 21: 't', 22: 'v', 23: 'l', 24: 'y', 25: 'z', 26: 'b'}\n",
      "X_train shape: (4000, 200)\n",
      "X_test shape: (1000, 200)\n",
      "Build model...\n",
      "Train...\n",
      "18/25 [====================>.........] - ETA: 25s - loss: 1.0892 - accuracy: 0.3780"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    out = parseData(Masterdir,inputdatasetfilename,SEPERATOR,DATA_COLUMN,LABEL_COLUMN,LABELS)\n",
    "    X_train = out[0]\n",
    "    y_train = out[1]\n",
    "\n",
    "    #Creating character dictionaries and format conversion in progess...\n",
    "    out = convert_chartonum(mapping_num2char,mapping_char2num,X_train,MAXLEN)\n",
    "    mapping_num2char = out[1]\n",
    "    mapping_char2num = out[2]\n",
    "    MAX_FEATURES = out[3]\n",
    "    X_train = np.asarray(out[0])\n",
    "    y_train = np.asarray(y_train).flatten()\n",
    "    \n",
    "    #Splitting data into train and test...\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "    print('X_train shape:', X_train.shape)\n",
    "    print('X_test shape:', X_test.shape)\n",
    "    \n",
    "    #Creating LSTM Network...\n",
    "    model = BidirectionalLSTM(deepcopy(X_train),deepcopy(y_train),[MAX_FEATURES, MAXLEN, embedding_size,\\\n",
    "                 filter_length, nb_filter, pool_length, lstm_output_size, batch_size, \\\n",
    "                 number_of_epochs, numclasses, test_size])\n",
    "\n",
    "    #Evaluating model...\n",
    "    evaluate_model(X_test,deepcopy(y_test),model,batch_size,numclasses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee215a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
